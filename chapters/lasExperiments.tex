\section{Listen attend and spell experiments}
In this section experiments use the full listen attend and spell architecture. After verification of the listener using a CTC output layer, the listen attend and spell network is first tested using greedy decoding. In contrast to beam search greedy decoding, does not maintain several hypotheses, instead it works with the most likely label each time step.

\subsection{Testing the Listener}
A crucial part of the listen attend and spell architecture is formed by the listener. Before working with a fully-fledged las a CTC-layer will be attached to the listener.
The idea is to verify the implementation. If CTC can extract relevant information from the listener, the attend and spell code should be able to do the same in later experiments. In order to keep memory requirements manageable in later experiments with the same listener all LSTM cells where set up with 64 hidden units. As the listener layers are bidirectional this means 64 in each direction so the hidden dimension is 128 in total. This sum is important, because the feature dimension of the lstm outputs is concatenated for each time step. If not further action is taken the listener produces features with a dimension of two times the number of elements per lstm.
CTC runs the logits it is given trough a softmax layer to compute label probabilities. To function it must therefore be given a logit tensor, where the feature dimension is equal to the number of labels, the system is supposed to output. To meet this requirement an extra linear output layer has been added to the listener which maps the feature dimension to 40, as required.
\begin{figure}
\includestandalone[width=0.49\linewidth]{tikz/listenerCTC}
\includestandalone[width=0.49\linewidth]{tikz/listenerCTC810}
\caption{The training progress shown for the Listener with added CTC layer. The loss values show in blue and green have been scaled with $\frac{1}{100}$. On the right a closeup on the last two training process is shown.}
\label{fig:listenCTC}
\end{figure}
Figure~\ref{fig:listenCTC}, shows the optimization algorithms progress, as measured by trainig loss, validation loss and validation set decoding phoneme error rate. The training was stopped, when the decoding results where no longer improving. During testing a phoneme error rate of \texttt{0.268} was observed, which is a pretty good results compared to the twenty four percent error rate of a comparable full BLSTM architecture given that the pyramidal layer compressed the time dimension into half of its original size. During decoding the CTC beam width was once more set to 100.



\subsection{Greedy Decoding Experiments}
\label{sec:greedy}
Using the tested listener with 64 hidden lstm units per direction and one pyramidal layer, the CTC layer is replaced with attend and spell functions. For computational efficiency these functions have been implemented within a customized RNN cell. Using an RNN framework makes it possible to use optimized code to unroll the attend and spell
computations. Among other things dynamically unrolling the second part of the network this way ensures efficient memory utilization and sequence length management. Within the new cell the decoder state size was chosen to be 128, considering the fact that the listener outputs features of size 128, which in turn determines the context vectors to have this same dimension.  Hoping to provide sufficient memory to remember past context vectors the decoder state RNN was set to the same dimension.
The state and feature networks, $\phi$ and $\psi$ were given one hidden layer each, with a hidden dimension of 64. This choice was made mainly to conserve memory. During training the network output was used instead of the true target with a probability of 0.7.
\begin{figure}
\includestandalone[width=0.49\linewidth]{tikz/lasGreedyp07}
\includestandalone[width=0.49\linewidth]{tikz/lasGreedyp07e3540}
\caption{The training progress shown for the full las architecture with greedy decoding. The loss values show in blue and green have been scaled with $\frac{1}{100}$. On the right a closeup on the last two training process is shown.}
\label{fig:lasGreedy}
\end{figure}
Figure~\ref{fig:lasGreedy} shows an overview of the training process. When considering the last five epochs, the decoding error  ranges between 0.5 and 0.9. This means that in the best case half of the labels produced by the system must be modified in order to
get to the target sequence. Considering timit utterance \texttt{fmld0\_sx295} the folded transcription with additional start and end of sentence tokens is given by:
\begin{lstlisting}[caption={Targets}]
<sos>  sil  ih  f  sil  k  eh  r  l  sil  k  ah  m  z
       sil  t  ah  m  aa  r  ah  hh  ae  v  er  r  ey
       n  jh  f  er  m  iy  dx  iy  ng  ih  sil  t  uw  sil
<eos>
\end{lstlisting}
From the input features the las network decodes:
\begin{lstlisting}[caption={Network output}]
<sos>  sil  hh  ih  f  sil  k  er  r  ow  ow  sil  sil
       t  ah  m  aa  hh  hh  ae  v  er  r  r  n  n  sil
       f  er  er  m  iy  iy  iy  iy  iy  iy  iy iy  sil
       sil  t  uw  sil
<eos>
\end{lstlisting}
The decoding and target sequences clearly bead some resemblance. However significant errors do exist in the network output in particular in the last third. The phoneme sequence dx  iy  ng  ih is incorrectly transcribed as iy  iy  iy  iy  iy  iy  iy iy,
which has a large impact on the error. The levenstein distance between the two labellings is 21. Given that the target sequence contains 45 labels including the start and end of sentence tokens, the error rate of this example is 0.5, it is therefore slightly better than the average of $\approx 0.55$, which is the average decoding error rate over the entire validation set.

\subsection{The attention mechanism}
\begin{figure}
\centering
\includestandalone[width=0.49\linewidth]{tikz/alpha}
\includestandalone[width=0.42\linewidth]{tikz/align}
\caption{Plot of the alignment vectors computed by the network for all 45 labels assigned to timit utterance \texttt{fmld0\_sx295} (left), and alignments assigned by a human listener (right).}
\label{fig:fullAttention}
\end{figure}
In order to further verify the implementation of the speller all attention weights $\alpha$ (equation~\ref{eq:alphas}), have been extracted during decoding. The left side of figure~\ref{fig:fullAttention} shows these vectors in concatenated matrix form. The speller assigns 45 labels. During each decoding time step one label is produced, therefore the attention matrix has the same number of columns. The y-Axis shows compressed input time. This dimension depends on the length of the input utterance under consideration and the compression level, which is set by the number of pyramidal LSTM layers. The attention weights are normalized using a softmax function, hence all elements must sum to one. The scale shown next to figure~\ref{fig:fullAttention} therefore ends at one, values close to one indicate that the network is focusing on a single feature. Generally the attention weights should gradually cover all relevant features over time, this is definitely the case, as the attention weight is clustered around the matrix diagonal. 
Furthermore only a limited number of compressed frames can be relevant at any given time, ideally each vector should contain a sharp peak at the most relevant frame.   
The timit data corpus comes with alignments found by human listeners. The recoding of utterance \texttt{fmld0\_sx295} lasts for $3.018s$. During the feature computation one frame is placed every $0.01s$. Therefore this utterance will come with 302 frames. Using a listener with a single pyramidal layer 151 high level feature vectors will be computed for this utterance. The right side of figure~\ref{fig:fullAttention} shows the rescaled human alignment. The two plots show some resemblance, which indicates that the implemented attention mechanism is working properly. 
\begin{figure}
\centering
\includestandalone[height=0.2\paperheight]{tikz/alphaZoom}
\caption{Closeup on interesting parts of the first third of the attention matrix.}
\includestandalone[height=0.2\paperheight]{tikz/alphaZoom2}
\caption{Closeup on interesting parts of the second third of the attention matrix.}
\includestandalone[height=0.2\paperheight]{tikz/alphaZoom3}
\caption{Closeup on interesting parts of the last third of the attention matrix.}
\label{fig:attention3}
\end{figure}
With the parameters set as described the in the previous section the attention mechanism breaks down towards the end, which can be seen in the closeup shown in figure~\ref{fig:attention3}. When the speller produces the incorrect sequence of iys the attention weights are spread out over roughly 15 feature vectors. 


\subsection{The effect of the groundtruth selection probability during training}
In the previous experiment the groundtruth was only used instead of the network output in 0.3 percent of the cases. As the network output is often incorrect during training, less emphasis will be put on past outputs later during decoding. This section investigates
other values. The same las network is retrained for 10 epochs using output probabilities of $0.2,0.4,0.6,0.8$.
\begin{figure}
\includestandalone[width=0.49\linewidth]{tikz/LAS_no_reg_e10_p02}
\includestandalone[width=0.49\linewidth]{tikz/LAS_no_reg_e10_p04}
\includestandalone[width=0.49\linewidth]{tikz/LAS_no_reg_e10_p06}
\includestandalone[width=0.49\linewidth]{tikz/LAS_no_reg_e10_p08}
\caption{Repetitions of the same experiment with network output reuse probabilities $0.2, 0.4, 0.6, 0.8$. }
\label{fig:lasGreedy2468}
\end{figure}
Results are shown in figure ~\ref{fig:lasGreedy2468}. It can be observed, that decoding results improve when reduced emphasis is placed on past labels during training. This observation is confirmed by the phoneme error rates of $2.08, 1.97, 1.1168, 0.87$, which where observed on the validation set.
To explore the effect of a low reuse probability over longer training periods the experiment has been repeated one more time with a $0.5$ groundtruth probability over 40 epochs.
\begin{figure}
\includestandalone[width=0.49\linewidth]{tikz/LAS_no_reg_e40_p05}
\includestandalone[width=0.49\linewidth]{tikz/LAS_no_reg_e40_p05_3540}
\caption{Training full las over 40 epochs with a network output reuse probability of $0.5$}
\label{fig:lasGreedy05}
\end{figure}
Figure~\ref{fig:lasGreedy05} depicts the training process. In comparison to the experiment show in figure~\ref{fig:lasGreedy} the network does significantly better during training, but
this improvement does not translate into a better decoding performance.

Ideally one would like to observe the opposite. If the output labels where correct more often during decoding, the networks trained to rely on them should outperform those which where not. Based on the observations above we conclude that beam search should be implemented in order to obtain labels of higher quality during decoding, which in turn should be beneficial to networks, which learned to rely on past outputs.


\subsection{A second attend and spell cell type}
Looking at figure~\ref{fig:las} it is not perfectly clear weather the blocks after the context computations represent LSTM-cell or are simply part of the feedforward labeling network. Until now it has been assumed that no additional recurrent LSTM network after the context is part of LAS. In this section such an extra RNN will be investigated briefly. The attend and spell cell variation is shown in figure~\ref{fig:lasVariants}. The state of the extra memory cell is labeled with $\mathbf{d}$.
\begin{figure}
\centering
\includestandalone[width=0.5\linewidth]{tikz/asCellType2}
\caption{A different attend and spell cell configuration, featuring an additional post context RNN}
\label{fig:lasVariants}
\includestandalone[width=0.5\linewidth]{tikz/LAS_no_reg_e10_p07_post_context_rnn}
\caption{Learning curves over 10 epochs using greedy decoding and a training network output reuse probability of 0.7.}
\label{fig:variantResult}
\end{figure}
A learning curve over 10 epochs using a ground-truth probability of 0.3 with greedy decoding is shown in figure~\ref{fig:variantResult}. The results are slightly worse then what was observed with the original setup, despite the extra weights. This result indicated that the extra post context RNN has no additional value.
It can therefore be concluded that the decoder state $\mathbf{s}_i$ is sufficient to remember past context information.

\subsection{Beam-search and Dropout}
Two measures have been taken to increase system performance. First a beam of las labels and states is kept, in order to explore several labeling hypotheses as described in chapter~\ref{sec:beamsearch}. However the current version of the code does not use language model rescoring.

\begin{figure}
\centering
\includestandalone[width=0.8\linewidth]{tikz/las_dropout}
\caption{Listen attend and spell forward connections with added dropout. No recurrences are shown.}
\label{fig:dropout}
\end{figure}
Second dropout has been added as shown in figure~\ref{fig:dropout}. The rationale behind using dropout regularization is to allow larger networks to be trained longer without running into over-fitting problems. In comparison to the previous experiments all network parameters have been doubled. Which means that all BLSTM layers used 128 units, all LSTM layers used 265 and all feedforward layers again 128 units. Due to good results in previous experiments the network output reuse probability during training was kept at 0.6. 
\begin{figure}
\includestandalone[width=0.5\linewidth]{tikz/LAS_dropout0805_in00_p06_e120_double_loss}
\includestandalone[width=0.5\linewidth]{tikz/LAS_dropout0805_in00_p06_e120_double_error}
\caption{LAS learning curves with a listener LSTM using 128 units per direction and one PLSTM. The speller LSTM state size was set to 256. All feed-forward networks in the speller had 128 units per layer and two layers overall.}
\label{fig:dropBeamRes}
\end{figure}
The settings described above led to the learning curves shown in figure~\ref{fig:dropBeamRes}. The validation error drops consistently under 50 percent, without large outliers. Considering once more utterance \texttt{fmld0\_sx295}.
\begin{lstlisting}[caption={Targets}]
<sos>  sil  ih  f  sil  k  eh  r  l  sil  k  ah  m  z
       sil  t  ah  m  aa  r  ah  hh  ae  v  er  r  ey
       n  jh  f  er  m  iy  dx  iy  ng  ih  
       sil  t  uw  sil
<eos>
\end{lstlisting}
The network now decodes:
\begin{lstlisting}[caption={Network output}]
<sos> sil hh ih f sil k ih r ow sil k ah m sil 
	  sil t ah m aa aa hh hh v v er ey 
	  n n sil f f er m iy iy iy iy sil
	  sil t uw sil sil
<eos>
\end{lstlisting}
Which is considerably better than the result obtained earlier. The levenstein distance over target length ratio drops to 0.36 for this utterance. However over the entire validation set 0.45 has been measured. This result is considerably better than the 0.55 which where observed in section \ref{sec:greedy} using a much simpler model with greedy decoding and input noise regularization.  



