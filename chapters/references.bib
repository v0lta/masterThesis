@article{Agarwal2015,
author = {Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
file = {:home/moritz/Documents/Mendeley/Agarwal et al/Unknown/Agarwal et al. - 2015 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
year = {2015}
}
@article{Bengio2015,
abstract = {The phase diagram of electron-doped pnictides is studied varying the temperature, electronic density, and isotropic quenched disorder strength by means of computational techniques applied to a three-orbital ({\$}xz{\$}, {\$}yz{\$}, {\$}xy{\$}) spin-fermion model with lattice degrees of freedom. In experiments, chemical doping introduces disorder but in theoretical studies the relationship between electronic doping and the randomly located dopants, with their associated quenched disorder, is difficult to address. In this publication, the use of computational techniques allows us to study independently the effects of electronic doping, regulated by a global chemical potential, and impurity disorder at randomly selected sites. Surprisingly, our Monte Carlo simulations reveal that the fast reduction with doping of the N$\backslash$'eel {\$}T{\_}N{\$} and the structural {\$}T{\_}S{\$} transition temperatures, and the concomitant stabilization of a robust nematic state, is primarily controlled by the magnetic dilution associated with the in-plane isotropic disorder introduced by Fe substitution. In the doping range studied, changes in the Fermi Surface produced by electron doping affect only slightly both critical temperatures.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03099v2},
author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
doi = {10.1201/9781420049176},
eprint = {arXiv:1506.03099v2},
file = {:home/moritz/Documents/Mendeley/Bengio et al/arXiv/Bengio et al. - 2015 - Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.pdf:pdf},
isbn = {0849371813},
issn = {1941-6016},
journal = {arXiv},
pages = {1--9},
title = {{Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks}},
year = {2015}
}
@article{Bengio1993,
abstract = {The authors seek to train recurrent neural networks in order to map input sequences to output sequences, for applications in sequence recognition or production. Results are presented showing that learning long-term dependencies in such recurrent networks using gradient descent is a very difficult task. It is shown how this difficulty arises when robustly latching bits of information with certain attractors. The derivatives of the output at time t with respect to the unit activations at time zero tend rapidly to zero as t increases for most input values. In such a situation, simple gradient descent techniques appear inappropriate. The consideration of alternative optimization methods and architectures is suggested},
author = {Bengio, Yoshua and Frasconi, Paolo and Simard, Patrice},
doi = {10.1109/ICNN.1993.298725},
file = {:home/moritz/Documents/Mendeley/Bengio, Frasconi, Simard/IEEE International Conference on Neural Networks - Conference Proceedings/Bengio, Frasconi, Simard - 1993 - The problem of learning long-term dependencies in recurrent networks.pdf:pdf},
isbn = {0780309995},
issn = {10987576},
journal = {IEEE International Conference on Neural Networks - Conference Proceedings},
pages = {1183--1188},
title = {{The problem of learning long-term dependencies in recurrent networks}},
volume = {1993-Janua},
year = {1993}
}
@article{Chan2015,
abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1{\%} without a dictionary or a language model, and 10.3{\%} with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0{\%}.},
archivePrefix = {arXiv},
arxivId = {1508.01211},
author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
eprint = {1508.01211},
file = {:home/moritz/Documents/Mendeley/Chan et al/arXiv preprint/Chan et al. - 2015 - Listen, attend and spell.pdf:pdf},
journal = {arXiv preprint},
pages = {1--16},
title = {{Listen, attend and spell}},
url = {http://arxiv.org/abs/1508.01211},
year = {2015}
}
@article{Chorowski2015,
abstract = {Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis and image caption generation. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation reaches a competitive 18.6$\backslash${\%} phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18$\backslash${\%} PER in single utterances and 20$\backslash${\%} in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6$\backslash${\%} level.},
archivePrefix = {arXiv},
arxivId = {1506.07503},
author = {Chorowski, Jan K and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1016/j.asr.2015.02.035},
eprint = {1506.07503},
file = {:home/moritz/Documents/Mendeley/Chorowski et al/Advances in Neural Information Processing Systems 28/Chorowski et al. - 2015 - Attention-Based Models for Speech Recognition.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 28},
pages = {577--585},
title = {{Attention-Based Models for Speech Recognition}},
url = {http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf},
year = {2015}
}
@misc{Colah2015,
author = {{Christopher Olah}},
booktitle = {Colahs Blog},
title = {{Understanding LSTM Networks}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
year = {2015}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
eprint = {1603.07285},
file = {:home/moritz/Documents/Mendeley/Dumoulin, Visin/Unknown/Dumoulin, Visin - 2016 - A guide to convolution arithmetic for deep learning.pdf:pdf},
pages = {1--28},
title = {{A guide to convolution arithmetic for deep learning}},
url = {http://arxiv.org/abs/1603.07285},
year = {2016}
}
@article{Graves2013b,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.5778v1},
author = {Graves, A and Mohamed, A.-R. and Hinton, G},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {arXiv:1303.5778v1},
file = {:home/moritz/Documents/Mendeley/Graves, Mohamed, Hinton/2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)/Graves, Mohamed, Hinton - 2013 - Speech recognition with deep recurrent neural networks.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
number = {6},
pages = {6645--6649},
title = {{Speech recognition with deep recurrent neural networks}},
url = {files/543/Graves{\_}et{\_}al-2013-Speech{\_}recognition{\_}with{\_}deep{\_}recurrent{\_}neural{\_}networks.pdf},
year = {2013}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {arXiv:1308.0850v5},
file = {:home/moritz/Documents/Mendeley/Graves/arXiv preprint arXiv1308.0850/Graves - 2013 - Generating sequences with recurrent neural networks.pdf:pdf},
isbn = {2000201075},
issn = {18792782},
journal = {arXiv preprint arXiv:1308.0850},
pages = {1--43},
pmid = {23459267},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Graves2004,
abstract = {Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) are local in space and time and closely related to a biological model of memory in the prefrontal cortex. Not only are they more biologically plausible than previous artificial RNNs, they also outperformed them on many artificially generated sequential processing tasks. This encouraged us to apply LSTM to more realistic problems, such as the recognition of spoken digits. Without any modification of the underlying algorithm, we achieved results comparable to state-of-the-art Hidden Markov Model (HMM) based recognisers on both the TIDIGITS and TI46 speech corpora. We conclude that LSTM should be further investigated as a biologically plausible basis for a bottom-up, neural net-based approach to speech recognition.},
author = {Graves, Alex and Eck, Douglas and Beringer, Nicole and Schmidhuber, Juergen},
doi = {10.1007/978-3-540-27835-1_10},
file = {:home/moritz/Documents/Mendeley/Graves et al/First International Workshop, BioADIT2004/Graves et al. - 2004 - Biologically Plausible Speech Recognition with LSTM Neural Nets.pdf:pdf},
isbn = {978-3-540-23339-8},
issn = {03029743},
journal = {First International Workshop, BioADIT2004},
pages = {127--136},
title = {{Biologically Plausible Speech Recognition with LSTM Neural Nets}},
year = {2004}
}
@inproceedings{Graves2013a,
abstract = {Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We find that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.},
author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel Rahman},
booktitle = {2013 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013 - Proceedings},
doi = {10.1109/ASRU.2013.6707742},
file = {:home/moritz/Documents/Mendeley/Graves, Jaitly, Mohamed/2013 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013 - Proceedings/Graves, Jaitly, Mohamed - 2013 - Hybrid speech recognition with Deep Bidirectional LSTM.pdf:pdf},
isbn = {9781479927562},
keywords = {DBLSTM,HMM-RNN hybrid},
pages = {273--278},
title = {{Hybrid speech recognition with Deep Bidirectional LSTM}},
year = {2013}
}
@article{Graves2005,
abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. ?? 2005 Elsevier Ltd. All rights reserved.},
author = {Graves, Alex and Schmidhuber, J??rgen},
doi = {10.1109/IJCNN.2005.1556215},
file = {:home/moritz/Documents/Mendeley/Graves, Schmidhuber/Proceedings of the International Joint Conference on Neural Networks/Graves, Schmidhuber - 2005 - Framewise phoneme classification with bidirectional LSTM networks.pdf:pdf},
isbn = {0780390482},
issn = {08936080},
journal = {Proceedings of the International Joint Conference on Neural Networks},
pages = {2047--2052},
pmid = {16112549},
title = {{Framewise phoneme classification with bidirectional LSTM networks}},
volume = {4},
year = {2005}
}
@article{Greff2015,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutn{\'{i}}k, Jan and Steunebrink, Bas R and Schmidhuber, J{\"{u}}rgen},
doi = {10.1017/CBO9781107415324.004},
eprint = {1503.04069},
file = {:home/moritz/Documents/Mendeley/Greff et al/arXiv/Greff et al. - 2015 - LSTM A Search Space Odyssey.pdf:pdf},
isbn = {9788578110796},
issn = {19454589},
journal = {arXiv},
pages = {10},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
url = {http://arxiv.org/abs/1503.04069},
year = {2015}
}
@article{Hannun2014,
abstract = {We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a “phoneme.” Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0{\%} error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
archivePrefix = {arXiv},
arxivId = {1412.5567v2},
author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
doi = {arXiv:1412.5567v2},
eprint = {1412.5567v2},
file = {:home/moritz/Documents/Mendeley/Hannun et al/Arxiv/Hannun et al. - 2014 - Deep Speech Scaling up end-to-end speech recognition.pdf:pdf},
journal = {Arxiv},
pages = {1--12},
title = {{Deep Speech: Scaling up end-to-end speech recognition}},
year = {2014}
}
@article{Hinton2012,
abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMMfits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed- forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.},
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
doi = {10.1109/MSP.2012.2205597},
file = {:home/moritz/Documents/Mendeley/Hinton et al/IEEE Signal Processing Magazine/Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Speech Recognition.pdf:pdf},
isbn = {1053-5888},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
number = {6},
pages = {82--97},
pmid = {13057166},
title = {{Deep Neural Networks for Acoustic Modeling in Speech Recognition}},
volume = {29},
year = {2012}
}
@article{Hochreiter1998,
abstract = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
author = {Hochreiter, Sepp},
doi = {10.1142/S0218488598000094},
file = {:home/moritz/Documents/Mendeley/Hochreiter/International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems/Hochreiter - 1998 - The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions.pdf:pdf},
isbn = {0218-4885},
issn = {0218-4885},
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
keywords = {long,long-term dependencies,recurrent neural nets,vanishing gradient},
number = {02},
pages = {107--116},
title = {{The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions}},
volume = {06},
year = {1998}
}
@article{Hochreiter1995,
abstract = {"Recurrent backprop" for learning to store information over extended time periods takes too long. The main reason is insufficient, decaying error back flow. We describe a novel, efficient "Long Short Term Memory" (LSTM) that overcomes this and related problems. Unlike previous approaches, LSTM can learn to bridge arbitrary time lags by enforcing constant error flow. Using gradient descent, LSTM explicitly learns when to store information and when to access it. In experimental comparisons with "Real-Time Recurrent Learning", "Recurrent Cascade-Correlation", "Elman nets", and "Neural Sequence Chunking", LSTM leads to many more successful runs, and learns much faster. Unlike its competitors, LSTM can solve tasks involving minimal time lags of more than 1000 time steps, even in noisy environments.},
author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
file = {:home/moritz/Documents/Mendeley/Hochreiter, Schmidhuber/Technical Report FKI-207-95/Hochreiter, Schmidhuber - 1995 - LONG SHORT TERM MEMORY.pdf:pdf},
journal = {Technical Report FKI-207-95},
pages = {1--8},
title = {{LONG SHORT TERM MEMORY}},
year = {1995}
}
@book{Huang2001,
abstract = {From the Publisher:New advances in spoken language processing: theory and practice In-depth coverage of speech processing, speech recognition, speech synthesis, spoken language understanding, and speech interface design Many case studies from state-of-the-art systems, including examples from Microsoft's advanced research labs Spoken Language Processing draws on the latest advances and techniques from multiple fields: computer science, electrical engineering, acoustics, linguistics, mathematics, psychology, and beyond. Starting with the fundamentals, it presents all this and more: Essential background on speech production and perception, probability and information theory, and pattern recognition Extracting information from the speech signal: useful representations and practical compression solutions Modern speech recognition techniques: hidden Markov models, acoustic and language modeling, improving resistance to environmental noises, search algorithms, and large vocabulary speech recognition Text-to-speech: analyzing documents, pitch and duration controls; trainable synthesis, and more Spoken language understanding: dialog management, spoken language applications, and multimodal interfaces To illustrate the book's methods, the authors present detailed case studies based on state-of-the-art systems, including Microsoft's Whisper speech recognizer, Whistler text-to-speech system, Dr. Who dialog system, and the MiPad handheld device. Whether you're planning, designing, building, or purchasing spoken language technology, this is the state of the artfromalgorithms through business productivity.},
author = {Huang, Xuedong and Acero, Alex and Hon, Hsiao-Wuen},
booktitle = {Processing},
isbn = {0130226165},
pages = {933},
title = {{Spoken Language Processing: A Guide to Theory, Algorithm and System Development}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/0130226165},
year = {2001}
}
@article{Juang1987,
author = {Juang, B H and Rabiner, L R and Wilpon, J G},
file = {:home/moritz/Documents/Mendeley/Juang, Rabiner, Wilpon/IEEE Transactions on Acoustics, Speech, and Signal Processing/Juang, Rabiner, Wilpon - 1987 - On the use of bandpass filtering in speech recognition.pdf:pdf},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
number = {7},
title = {{On the use of bandpass filtering in speech recognition}},
volume = {ASSP-35},
year = {1987}
}
@article{LeCun1998,
abstract = {A long and detailed paper on convolutional nets, graph transformer$\backslash$nnetworks, and discriminative training methods for sequence labeling.$\backslash$nWe show how to build systems that integrate segmentation, feature$\backslash$nextraction, classification, contextual post-processing, and language$\backslash$nmodeling into one single learning machine trained end-to-end. Applications$\backslash$nto handwriting recognition and face detection are described.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:home/moritz/Documents/Mendeley/LeCun et al/Proceedings of the IEEE/LeCun et al. - 1998 - Gradient Based Learning Applied to Document Recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {character recognition,convolutional neural networks,document recog-,finite state transducers,gradient-based learning,graph,machine learning,neural networks,nition,ocr,optical,transformer networks},
number = {11},
pages = {2278--2324},
pmid = {15823584},
title = {{Gradient Based Learning Applied to Document Recognition}},
volume = {86},
year = {1998}
}
@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/moritz/Documents/Mendeley/Pascanu, Mikolov, Bengio/Proceedings of The 30th International Conference on Machine Learning/Pascanu, Mikolov, Bengio - 2012 - Understanding the exploding gradient problem.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {Proceedings of The 30th International Conference on Machine Learning},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{Understanding the exploding gradient problem}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}
@book{Rojas1996,
author = {Rojas, Raul},
doi = {10.1109/78.127967},
file = {:home/moritz/Documents/Mendeley/Rojas/Unknown/Rojas - 1996 - Neural Networks - A Systematic Introduction - Backpropagation.pdf:pdf},
isbn = {9783540605058},
issn = {1053587X},
pages = {152 -- 184},
publisher = {Springer Berlin Heidelberg},
title = {{Neural Networks - A Systematic Introduction - Backpropagation}},
url = {https://page.mi.fu-berlin.de/rojas/neural/neuron.pdf},
year = {1996}
}
@article{Sak2014,
abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1128v1},
author = {Sak, Ha$\backslash$csim and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
doi = {arXiv:1402.1128},
eprint = {arXiv:1402.1128v1},
file = {:home/moritz/Documents/Mendeley/Sak, Senior, Beaufays/arXiv preprint arXiv1402.1128/Sak, Senior, Beaufays - 2014 - Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recogniti.pdf:pdf},
journal = {arXiv preprint arXiv:1402.1128},
number = {Cd},
title = {{Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}},
year = {2014}
}
@article{Srivastava2014,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:home/moritz/Documents/Mendeley/Srivastava et al/Unknown/Srivastava et al. - 2014 - Dropout prevent NN from overfitting.pdf:pdf},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: prevent NN from overfitting}},
volume = {15},
year = {2014}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
eprint = {1409.3215},
file = {:home/moritz/Documents/Mendeley/Sutskever, Vinyals, Le/Advances in Neural Information Processing Systems (NIPS)/Sutskever, Vinyals, Le - 2014 - Sequence to sequence learning with neural networks.pdf:pdf},
isbn = {1409.3215},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {3104--3112},
title = {{Sequence to sequence learning with neural networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/moritz/Documents/Mendeley/Tobergte, Curtis/Journal of Chemical Information and Modeling/Tobergte, Curtis - 2013 - CONVOLUTIONAL, LONG SHORT-TERM MEMORY, FULLY CONNECTED DEEP NEURAL NETWORKS.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{CONVOLUTIONAL, LONG SHORT-TERM MEMORY, FULLY CONNECTED DEEP NEURAL NETWORKS}},
volume = {53},
year = {2013}
}
@article{Werbos2006,
abstract = {Backwards calculation of derivatives -- sometimes called the reverse$\backslash$nmode, the full adjoint method, or backpropagation -- has been developed$\backslash$nand applied in many fields. This paper reviews several strands of$\backslash$nhistory, advanced capabilities and types of application -- particularly$\backslash$nthose which are crucial to the development of brain-like capabilities$\backslash$nin intelligent control and artificial intelligence.},
author = {Werbos, Paul J.},
doi = {10.1007/3-540-28438-9_2},
file = {:home/moritz/Documents/Mendeley/Werbos/Lecture Notes in Computational Science and Engineering/Werbos - 2006 - Backwards Differentiation in AD and Neural Nets Past Links and New Opportunities.pdf:pdf},
isbn = {9783540284031},
issn = {14397358},
journal = {Lecture Notes in Computational Science and Engineering},
keywords = {Adjoint,Approximate dynamic programming,Backpropagation,Implicit systems,Intelligent control,MLP,Neural networks,Recurrent networks,Reinforcement learning,Reverse mode},
pages = {15--34},
title = {{Backwards Differentiation in AD and Neural Nets: Past Links and New Opportunities}},
volume = {50},
year = {2006}
}
@article{Zaremba2014,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
annote = {class BasicLSTMCell(RNNCell), is based on this paper.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.2329v3},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
eprint = {arXiv:1409.2329v3},
file = {:home/moritz/Documents/Mendeley/Zaremba, Sutskever, Vinyals/arXiv1409.2329 cs/Zaremba, Sutskever, Vinyals - 2014 - Recurrent Neural Network Regularization.pdf:pdf},
isbn = {078036404X},
journal = {arXiv:1409.2329 [cs]},
number = {2013},
pages = {1--8},
title = {{Recurrent Neural Network Regularization}},
url = {http://arxiv.org/abs/1409.2329$\backslash$nhttp://www.arxiv.org/pdf/1409.2329.pdf},
year = {2014}
}
@techreport{Diehl2013,
address = {Leuven},
author = {Diehl, Moritz},
institution = {Optimization in Engineering Center (OPTEC) and Electrical Engineering Department (ESAT-SCD), KU Leuven},
pages = {148},
title = {{Script for Numerical Optimization Course B-KUL-H03E3A}},
year = {2013}
}
