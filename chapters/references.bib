@article{Agarwal2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.04467},
author = {Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {arXiv:1603.04467},
file = {:home/moritz/Documents/Mendeley/Agarwal et al/CoRR/Agarwal et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
journal = {CoRR},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
volume = {abs/1603.0},
year = {2016}
}
@article{Amodei2015,
abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech—two vastly different languages. Be- cause it replaces entire pipelines of hand-engineered components with neural net- works, end-to-end learning allows us to handle a diverse variety of speech includ- ing noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system [26]. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior ar- chitectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, deliver- ing low latency when serving users at scale.},
archivePrefix = {arXiv},
arxivId = {1512.02595},
author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
doi = {10.1145/1143844.1143891},
eprint = {1512.02595},
file = {:home/moritz/Documents/Mendeley/Amodei et al/Unknown/Amodei et al. - 2015 - Deep Speech 2 End-to-End Speech Recognition in English and Mandarin Baidu.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
pmid = {1000285842},
title = {{Deep Speech 2: End-to-End Speech Recognition in English and Mandarin Baidu}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143891},
year = {2015}
}
@article{Bahdanau2015,
abstract = {Neural machine translation is a recently proposed approach to machine transla- tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu- ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec- ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.0473v7},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {arXiv:1409.0473v7},
file = {:home/moritz/Documents/Mendeley/Bahdanau, Cho, Bengio/CoRR/Bahdanau, Cho, Bengio - 2014 - NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE.pdf:pdf},
journal = {CoRR},
pages = {1--15},
title = {{NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE}},
url = {http://arxiv.org/abs/1409.0473},
volume = {abs/1409.0},
year = {2014}
}
@book{Ben-david2014,
abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a princi- pled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Fol- lowing a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous text- books. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorith- mic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds.Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to stu- dents and nonexpert readers in statistics, computer science,mathematics, and engineering. Shai},
address = {Jerusalem},
author = {Ben-david, Shai},
file = {:home/moritz/Documents/Mendeley/Ben-david/Unknown/Ben-david - 2014 - Understanding Machine Learning From Theory to Algorithms.pdf:pdf},
isbn = {9781107057135},
pages = {449},
publisher = {Cambridge Cambridge university press},
title = {{Understanding Machine Learning: From Theory to Algorithms}},
url = {http://www.cs.huji.ac.il/{~}shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf},
year = {2014}
}
@article{Bengio2015,
abstract = {The phase diagram of electron-doped pnictides is studied varying the temperature, electronic density, and isotropic quenched disorder strength by means of computational techniques applied to a three-orbital ({\$}xz{\$}, {\$}yz{\$}, {\$}xy{\$}) spin-fermion model with lattice degrees of freedom. In experiments, chemical doping introduces disorder but in theoretical studies the relationship between electronic doping and the randomly located dopants, with their associated quenched disorder, is difficult to address. In this publication, the use of computational techniques allows us to study independently the effects of electronic doping, regulated by a global chemical potential, and impurity disorder at randomly selected sites. Surprisingly, our Monte Carlo simulations reveal that the fast reduction with doping of the N$\backslash$'eel {\$}T{\_}N{\$} and the structural {\$}T{\_}S{\$} transition temperatures, and the concomitant stabilization of a robust nematic state, is primarily controlled by the magnetic dilution associated with the in-plane isotropic disorder introduced by Fe substitution. In the doping range studied, changes in the Fermi Surface produced by electron doping affect only slightly both critical temperatures.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03099v2},
author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
doi = {10.1201/9781420049176},
eprint = {arXiv:1506.03099v2},
file = {:home/moritz/Documents/Mendeley/Bengio et al/arXiv/Bengio et al. - 2015 - Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.pdf:pdf},
isbn = {0849371813},
issn = {1941-6016},
journal = {arXiv},
pages = {1--9},
title = {{Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks}},
year = {2015}
}
@article{Bengio1993,
abstract = {The authors seek to train recurrent neural networks in order to map input sequences to output sequences, for applications in sequence recognition or production. Results are presented showing that learning long-term dependencies in such recurrent networks using gradient descent is a very difficult task. It is shown how this difficulty arises when robustly latching bits of information with certain attractors. The derivatives of the output at time t with respect to the unit activations at time zero tend rapidly to zero as t increases for most input values. In such a situation, simple gradient descent techniques appear inappropriate. The consideration of alternative optimization methods and architectures is suggested},
author = {Bengio, Yoshua and Frasconi, Paolo and Simard, Patrice},
doi = {10.1109/ICNN.1993.298725},
file = {:home/moritz/Documents/Mendeley/Bengio, Frasconi, Simard/IEEE International Conference on Neural Networks - Conference Proceedings/Bengio, Frasconi, Simard - 1993 - The problem of learning long-term dependencies in recurrent networks.pdf:pdf},
isbn = {0780309995},
issn = {10987576},
journal = {IEEE International Conference on Neural Networks - Conference Proceedings},
pages = {1183--1188},
title = {{The problem of learning long-term dependencies in recurrent networks}},
volume = {1993-Janua},
year = {1993}
}
@book{Bishop1995,
author = {Bishop, Christopher M.},
publisher = {Oxford University Press},
title = {{Neural Networks for Pattern Recognition}},
year = {1995}
}
@article{Chan2015,
abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1{\%} without a dictionary or a language model, and 10.3{\%} with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0{\%}.},
archivePrefix = {arXiv},
arxivId = {1508.01211},
author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
eprint = {1508.01211},
file = {:home/moritz/Documents/Mendeley/Chan et al/arXiv preprint/Chan et al. - 2015 - Listen, attend and spell.pdf:pdf},
journal = {arXiv preprint},
pages = {1--16},
title = {{Listen, attend and spell}},
url = {http://arxiv.org/abs/1508.01211},
year = {2015}
}
@article{Chorowski2015,
abstract = {Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis and image caption generation. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation reaches a competitive 18.6$\backslash${\%} phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18$\backslash${\%} PER in single utterances and 20$\backslash${\%} in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6$\backslash${\%} level.},
archivePrefix = {arXiv},
arxivId = {1506.07503},
author = {Chorowski, Jan K and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1016/j.asr.2015.02.035},
eprint = {1506.07503},
file = {:home/moritz/Documents/Mendeley/Chorowski et al/Advances in Neural Information Processing Systems 28/Chorowski et al. - 2015 - Attention-Based Models for Speech Recognition.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 28},
pages = {577--585},
title = {{Attention-Based Models for Speech Recognition}},
url = {http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf},
year = {2015}
}
@article{Chorowski2014,
abstract = {We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context created with a subset of input symbols elected by the attention mechanism. We report initial results demonstrating that this new approach achieves phoneme error rates that are comparable to the state-of-the-art HMM-based decoders, on the TIMIT dataset.},
archivePrefix = {arXiv},
arxivId = {1412.1602},
author = {Chorowski, Jan and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1412.1602},
file = {:home/moritz/Documents/Mendeley/Chorowski et al/Deep Learning and Representation Learning Workshop, NIPS 2014/Chorowski et al. - 2014 - End-to-end Continuous Speech Recognition using Attention-based Recurrent NN First Results.pdf:pdf},
journal = {Deep Learning and Representation Learning Workshop, NIPS 2014},
pages = {1--10},
title = {{End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results}},
url = {http://arxiv.org/abs/1412.1602},
year = {2014}
}
@article{Chorowski2014a,
author = {Chorowski, Jan and Serdyuk, Dmitriy and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
file = {:home/moritz/Documents/Mendeley/Chorowski et al/Unknown/Chorowski et al. - 2014 - Attention-Based Models for Speech Recognition.pdf:pdf},
pages = {1--9},
title = {{Attention-Based Models for Speech Recognition}},
year = {2014}
}
@misc{Colah2015,
author = {{Christopher Olah}},
booktitle = {Colahs Blog},
title = {{Understanding LSTM Networks}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
year = {2015}
}
@techreport{Diehl2013,
address = {Leuven},
author = {Diehl, Moritz},
institution = {Optimization in Engineering Center (OPTEC) and Electrical Engineering Department (ESAT-SCD), KU Leuven},
pages = {148},
title = {{Script for Numerical Optimization Course B-KUL-H03E3A}},
year = {2013}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
eprint = {1603.07285},
file = {:home/moritz/Documents/Mendeley/Dumoulin, Visin/Unknown/Dumoulin, Visin - 2016 - A guide to convolution arithmetic for deep learning.pdf:pdf},
pages = {1--28},
title = {{A guide to convolution arithmetic for deep learning}},
url = {http://arxiv.org/abs/1603.07285},
year = {2016}
}
@misc{Garofolo1993,
abstract = {The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The TIMIT corpus includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16kHz speech waveform file for each utterance. Corpus design was a joint effort among the Massachusetts Institute of Technology (MIT), SRI International (SRI) and Texas Instruments, Inc. (TI). The speech was recorded at TI, transcribed at MIT and verified and prepared for CD-ROM production by the National Institute of Standards and Technology (NIST). The TIMIT corpus transcriptions have been hand verified. Test and training subsets, balanced for phonetic and dialectal coverage, are specified. Tabular computer-searchable information is included as well as written documentation.},
address = {Philadelphia},
author = {Garofolo, John and Lamel, Lori and Fisher, William and Fiscus, Jonathan and Pallett, David and Dahlgren, Nancy and Zue, Victor},
booktitle = {LDC93S1},
publisher = {Linguistic Data Consortium},
title = {{TIMIT Acoustic-Phonetic Continuous Speech Corpus}},
url = {https://catalog.ldc.upenn.edu/ldc93s1},
year = {1993}
}
@article{Golik2013,
abstract = {In this paper we investigate the error criteria that are optimized during the training of artificial neural networks (ANN). We compare the bounds of the squared error (SE) and the crossentropy (CE) criteria being the most popular choices in stateof- The art implementations. The evaluation is performed on automatic speech recognition (ASR) and handwriting recognition (HWR) tasks using a hybrid HMM-ANN model. We find that with randomly initialized weights, the squared error based ANN does not converge to a good local optimum. However, with a good initialization by pre-training, the word error rate of our best CE trained system could be reduced from 30.9{\%} to 30.5{\%} on the ASR, and from 22.7{\%} to 21.9{\%} on the HWR task by performing a few additional "fine-tuning" iterations with the SE criterion. Copyright {\textcopyright} 2013 ISCA.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01842v1},
author = {Golik, Pavel and Doetsch, Patrick and Ney, Hermann},
doi = {10.1145/1102351.1102422},
eprint = {arXiv:1503.01842v1},
file = {:home/moritz/Documents/Mendeley/Golik, Doetsch, Ney/Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH/Golik, Doetsch, Ney - 2013 - Cross-entropy vs. Squared error training A theoretical and experimental comparison.pdf:pdf},
isbn = {1595931805},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Automatic speech recognition,Handwriting recognition,Hybrid approach,Training criterion for ANN training},
number = {2},
pages = {1756--1760},
title = {{Cross-entropy vs. Squared error training: A theoretical and experimental comparison}},
volume = {2},
year = {2013}
}
@article{Graves2013b,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.5778v1},
author = {Graves, A and Mohamed, A.-R. and Hinton, G},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {arXiv:1303.5778v1},
file = {:home/moritz/Documents/Mendeley/Graves, Mohamed, Hinton/2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)/Graves, Mohamed, Hinton - 2013 - Speech recognition with deep recurrent neural networks.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
number = {6},
pages = {6645--6649},
title = {{Speech recognition with deep recurrent neural networks}},
url = {files/543/Graves{\_}et{\_}al-2013-Speech{\_}recognition{\_}with{\_}deep{\_}recurrent{\_}neural{\_}networks.pdf},
year = {2013}
}
@book{Graves2008,
abstract = {Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to lo- calised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. Long short-term memory is an especially promising recurrent archi- tecture, able to bridge long time delays between relevant input and output events, and thereby access long range context. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks in general, and long short-term memory in particular. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the align- ment between the inputs and the labels is unknown, and (2) an extension of long short-term memory to multidimensional data, such as images and video sequences. Experimental results are presented on speech recognition, online and offline handwriting recognition, keyword spotting, image segmen- tation and image classification, demonstrating the advantages of advanced recurrent networks over other sequential algorithms, such as hidden Markov Models},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v1},
author = {Graves, Alex},
booktitle = {Technische Universit{\{}{\"{a}}{\}}t M{\{}{\"{u}}{\}}nchen Fakult{\{}{\"{a}}{\}}t f{\{}{\"{u}}{\}}r Informatik},
doi = {10.1007/978-3-642-24797-2},
eprint = {arXiv:1308.0850v1},
file = {:home/moritz/Documents/Mendeley/Graves/Technische Universit{\"{a}}t M{\"{u}}nchen Fakult{\"{a}}t f{\"{u}}r Informatik/Graves - 2008 - Supervised Sequence Labelling with Recurrent Neural Networks.pdf:pdf},
institution = {Technische Universitaet Muenchen},
isbn = {2000201075},
issn = {01406736},
pages = {124},
pmid = {7491034},
title = {{Supervised Sequence Labelling with Recurrent Neural Networks, Phd Thesis}},
url = {http://scholar.google.com/scholar?hl=en{\%}7B{\&}{\%}7DbtnG=Search{\%}7B{\&}{\%}7Dq=intitle:with+Recurrent+Neural+Networks{\%}7B{\#}{\%}7D0{\%}7B{\%}25{\%}7D5Cnhttp://books.google.com/books?hl=en{\%}7B{\&}{\%}7Dlr={\%}7B{\&}{\%}7Did=4UauNDGQWN4C{\%}7B{\&}{\%}7Doi=fnd{\%}7B{\&}{\%}7Dpg=PR1{\%}7B{\&}{\%}7Ddq=Supervised+Sequence+Labellin},
year = {2008}
}
@book{Graves2012,
abstract = {Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to lo- calised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. Long short-term memory is an especially promising recurrent archi- tecture, able to bridge long time delays between relevant input and output events, and thereby access long range context. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks in general, and long short-term memory in particular. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the align- ment between the inputs and the labels is unknown, and (2) an extension of long short-term memory to multidimensional data, such as images and video sequences. Experimental results are presented on speech recognition, online and offline handwriting recognition, keyword spotting, image segmen- tation and image classification, demonstrating the advantages of advanced recurrent networks over other sequential algorithms, such as hidden Markov Models},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v1},
author = {Graves, Alex},
booktitle = {Springer Handbook of Computational Intelligence},
doi = {10.1007/978-3-642-24797-2},
eprint = {arXiv:1308.0850v1},
file = {:home/moritz/Documents/Mendeley/Graves/Springer Handbook of Computational Intelligence/Graves - 2012 - Supervised Sequence Labelling with Recurrent Neural Networks.pdf:pdf},
isbn = {2000201075},
issn = {01406736},
pages = {124},
pmid = {7491034},
publisher = {Springer Berlin Heidelberg},
title = {{Supervised Sequence Labelling with Recurrent Neural Networks}},
url = {http://scholar.google.com/scholar?hl=en{\%}7B{\&}{\%}7DbtnG=Search{\%}7B{\&}{\%}7Dq=intitle:with+Recurrent+Neural+Networks{\%}7B{\#}{\%}7D0{\%}7B{\%}25{\%}7D5Cnhttp://books.google.com/books?hl=en{\%}7B{\&}{\%}7Dlr={\%}7B{\&}{\%}7Did=4UauNDGQWN4C{\%}7B{\&}{\%}7Doi=fnd{\%}7B{\&}{\%}7Dpg=PR1{\%}7B{\&}{\%}7Ddq=Supervised+Sequence+Labellin},
year = {2012}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {arXiv:1308.0850v5},
file = {:home/moritz/Documents/Mendeley/Graves/arXiv preprint arXiv1308.0850/Graves - 2013 - Generating sequences with recurrent neural networks.pdf:pdf},
isbn = {2000201075},
issn = {18792782},
journal = {arXiv preprint arXiv:1308.0850},
pages = {1--43},
pmid = {23459267},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Graves2004,
abstract = {Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) are local in space and time and closely related to a biological model of memory in the prefrontal cortex. Not only are they more biologically plausible than previous artificial RNNs, they also outperformed them on many artificially generated sequential processing tasks. This encouraged us to apply LSTM to more realistic problems, such as the recognition of spoken digits. Without any modification of the underlying algorithm, we achieved results comparable to state-of-the-art Hidden Markov Model (HMM) based recognisers on both the TIDIGITS and TI46 speech corpora. We conclude that LSTM should be further investigated as a biologically plausible basis for a bottom-up, neural net-based approach to speech recognition.},
author = {Graves, Alex and Eck, Douglas and Beringer, Nicole and Schmidhuber, Juergen},
doi = {10.1007/978-3-540-27835-1_10},
file = {:home/moritz/Documents/Mendeley/Graves et al/First International Workshop, BioADIT2004/Graves et al. - 2004 - Biologically Plausible Speech Recognition with LSTM Neural Nets.pdf:pdf},
isbn = {978-3-540-23339-8},
issn = {03029743},
journal = {First International Workshop, BioADIT2004},
pages = {127--136},
title = {{Biologically Plausible Speech Recognition with LSTM Neural Nets}},
year = {2004}
}
@article{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
doi = {10.1145/1143844.1143891},
file = {:home/moritz/Documents/Mendeley/Graves et al/Proceedings of the 23rd international conference on Machine Learning/Graves et al. - 2006 - Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
title = {{Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
year = {2006}
}
@inproceedings{Graves2013a,
abstract = {Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We find that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.},
author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel Rahman},
booktitle = {2013 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013 - Proceedings},
doi = {10.1109/ASRU.2013.6707742},
file = {:home/moritz/Documents/Mendeley/Graves, Jaitly, Mohamed/2013 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013 - Proceedings/Graves, Jaitly, Mohamed - 2013 - Hybrid speech recognition with Deep Bidirectional LSTM.pdf:pdf},
isbn = {9781479927562},
keywords = {DBLSTM,HMM-RNN hybrid},
pages = {273--278},
title = {{Hybrid speech recognition with Deep Bidirectional LSTM}},
year = {2013}
}
@article{Graves2005,
abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. ?? 2005 Elsevier Ltd. All rights reserved.},
author = {Graves, Alex and Schmidhuber, J??rgen},
doi = {10.1109/IJCNN.2005.1556215},
file = {:home/moritz/Documents/Mendeley/Graves, Schmidhuber/Proceedings of the International Joint Conference on Neural Networks/Graves, Schmidhuber - 2005 - Framewise phoneme classification with bidirectional LSTM networks.pdf:pdf},
isbn = {0780390482},
issn = {08936080},
journal = {Proceedings of the International Joint Conference on Neural Networks},
pages = {2047--2052},
pmid = {16112549},
title = {{Framewise phoneme classification with bidirectional LSTM networks}},
volume = {4},
year = {2005}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory re-sources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demon-strate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.5401v2},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {arXiv:1410.5401v2},
file = {:home/moritz/Documents/Mendeley/Graves, Wayne, Danihelka/Arxiv/Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf:pdf},
journal = {Arxiv},
pages = {1--26},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
year = {2014}
}
@article{Greff2015,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutn{\'{i}}k, Jan and Steunebrink, Bas R and Schmidhuber, J{\"{u}}rgen},
doi = {10.1017/CBO9781107415324.004},
eprint = {1503.04069},
file = {:home/moritz/Documents/Mendeley/Greff et al/arXiv/Greff et al. - 2015 - LSTM A Search Space Odyssey.pdf:pdf},
isbn = {9788578110796},
issn = {19454589},
journal = {arXiv},
pages = {10},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
url = {http://arxiv.org/abs/1503.04069},
year = {2015}
}
@article{HAN2006,
author = {HAN, Wei and CHAN, Cheong-Fat and CHOY, Chiu-Sing and PUN, Kong-Pang},
file = {:home/moritz/Documents/Mendeley/HAN et al/IEEE/HAN et al. - 2006 - An Efficient MFCC Extraction Method in Speech Recognition.pdf:pdf},
isbn = {0780393902},
journal = {IEEE},
pages = {145--148},
title = {{An Efficient MFCC Extraction Method in Speech Recognition}},
year = {2006}
}
@article{Hannun2014,
abstract = {We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a “phoneme.” Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0{\%} error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
archivePrefix = {arXiv},
arxivId = {1412.5567v2},
author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
doi = {arXiv:1412.5567v2},
eprint = {1412.5567v2},
file = {:home/moritz/Documents/Mendeley/Hannun et al/Arxiv/Hannun et al. - 2014 - Deep Speech Scaling up end-to-end speech recognition.pdf:pdf},
journal = {Arxiv},
pages = {1--12},
title = {{Deep Speech: Scaling up end-to-end speech recognition}},
year = {2014}
}
@article{Hinton2012,
abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMMfits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed- forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.},
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
doi = {10.1109/MSP.2012.2205597},
file = {:home/moritz/Documents/Mendeley/Hinton et al/IEEE Signal Processing Magazine/Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Speech Recognition.pdf:pdf},
isbn = {1053-5888},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
number = {6},
pages = {82--97},
pmid = {13057166},
title = {{Deep Neural Networks for Acoustic Modeling in Speech Recognition}},
volume = {29},
year = {2012}
}
@article{Hochreiter1998,
abstract = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
author = {Hochreiter, Sepp},
doi = {10.1142/S0218488598000094},
file = {:home/moritz/Documents/Mendeley/Hochreiter/International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems/Hochreiter - 1998 - The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions.pdf:pdf},
isbn = {0218-4885},
issn = {0218-4885},
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
keywords = {long,long-term dependencies,recurrent neural nets,vanishing gradient},
number = {02},
pages = {107--116},
title = {{The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions}},
volume = {06},
year = {1998}
}
@article{Hochreiter1995,
abstract = {"Recurrent backprop" for learning to store information over extended time periods takes too long. The main reason is insufficient, decaying error back flow. We describe a novel, efficient "Long Short Term Memory" (LSTM) that overcomes this and related problems. Unlike previous approaches, LSTM can learn to bridge arbitrary time lags by enforcing constant error flow. Using gradient descent, LSTM explicitly learns when to store information and when to access it. In experimental comparisons with "Real-Time Recurrent Learning", "Recurrent Cascade-Correlation", "Elman nets", and "Neural Sequence Chunking", LSTM leads to many more successful runs, and learns much faster. Unlike its competitors, LSTM can solve tasks involving minimal time lags of more than 1000 time steps, even in noisy environments.},
author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
file = {:home/moritz/Documents/Mendeley/Hochreiter, Schmidhuber/Technical Report FKI-207-95/Hochreiter, Schmidhuber - 1995 - LONG SHORT TERM MEMORY.pdf:pdf},
journal = {Technical Report FKI-207-95},
pages = {1--8},
title = {{LONG SHORT TERM MEMORY}},
year = {1995}
}
@book{Huang2001,
abstract = {From the Publisher:New advances in spoken language processing: theory and practice In-depth coverage of speech processing, speech recognition, speech synthesis, spoken language understanding, and speech interface design Many case studies from state-of-the-art systems, including examples from Microsoft's advanced research labs Spoken Language Processing draws on the latest advances and techniques from multiple fields: computer science, electrical engineering, acoustics, linguistics, mathematics, psychology, and beyond. Starting with the fundamentals, it presents all this and more: Essential background on speech production and perception, probability and information theory, and pattern recognition Extracting information from the speech signal: useful representations and practical compression solutions Modern speech recognition techniques: hidden Markov models, acoustic and language modeling, improving resistance to environmental noises, search algorithms, and large vocabulary speech recognition Text-to-speech: analyzing documents, pitch and duration controls; trainable synthesis, and more Spoken language understanding: dialog management, spoken language applications, and multimodal interfaces To illustrate the book's methods, the authors present detailed case studies based on state-of-the-art systems, including Microsoft's Whisper speech recognizer, Whistler text-to-speech system, Dr. Who dialog system, and the MiPad handheld device. Whether you're planning, designing, building, or purchasing spoken language technology, this is the state of the artfromalgorithms through business productivity.},
address = {Redmond},
author = {Huang, Xuedong and Acero, Alex and Hon, Hsiao-Wuen},
booktitle = {Processing},
isbn = {0130226165},
pages = {933},
publisher = {Prentice Hall PTR},
title = {{Spoken Language Processing: A Guide to Theory, Algorithm and System Development}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/0130226165},
year = {2001}
}
@article{Juang1987,
author = {Juang, B H and Rabiner, L R and Wilpon, J G},
file = {:home/moritz/Documents/Mendeley/Juang, Rabiner, Wilpon/IEEE Transactions on Acoustics, Speech, and Signal Processing/Juang, Rabiner, Wilpon - 1987 - On the use of bandpass filtering in speech recognition.pdf:pdf},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
number = {7},
title = {{On the use of bandpass filtering in speech recognition}},
volume = {ASSP-35},
year = {1987}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo- ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre- tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con- vergence properties of the algorithm and provide a regret bound on the conver- gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v8},
author = {Kingma, Diederik P and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v8},
file = {:home/moritz/Documents/Mendeley/Kingma, Ba/Unknown/Kingma, Ba - 2015 - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.pdf:pdf},
pages = {1--15},
title = {{ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION}},
year = {2015}
}
@misc{Langmead2016,
author = {Langmead, Ben},
booktitle = {Johns Hopkins Whiting school of engineering},
file = {:home/moritz/Documents/Mendeley/Langmead/Johns Hopkins Whiting school of engineering/Langmead - 2016 - Dynamic programming and edit distance.pdf:pdf},
title = {{Dynamic programming and edit distance}},
url = {http://www.cs.jhu.edu/{~}langmea/resources/lecture{\_}notes/dp{\_}and{\_}edit{\_}dist.pdf},
urldate = {2016-12-12},
year = {2016}
}
@article{LeCun1998,
abstract = {A long and detailed paper on convolutional nets, graph transformer$\backslash$nnetworks, and discriminative training methods for sequence labeling.$\backslash$nWe show how to build systems that integrate segmentation, feature$\backslash$nextraction, classification, contextual post-processing, and language$\backslash$nmodeling into one single learning machine trained end-to-end. Applications$\backslash$nto handwriting recognition and face detection are described.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:home/moritz/Documents/Mendeley/LeCun et al/Proceedings of the IEEE/LeCun et al. - 1998 - Gradient Based Learning Applied to Document Recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {character recognition,convolutional neural networks,document recog-,finite state transducers,gradient-based learning,graph,machine learning,neural networks,nition,ocr,optical,transformer networks},
number = {11},
pages = {2278--2324},
pmid = {15823584},
title = {{Gradient Based Learning Applied to Document Recognition}},
volume = {86},
year = {1998}
}
@article{Lee1989,
abstract = {Hidden Markov modeling is extended to speaker-independent phone recognition. Using multiple codebooks of various linear-predictive-coding (LPC) parameters and discrete hidden Markov models (HMMs) the authors obtain a speaker-independent phone recognition accuracy of 58.8-73.8{\%} on the TIMIT database, depending on the type of acoustic and language models used. In comparison, the performance of expert spectrogram readers is only 69{\%} without use of higher level knowledge. The authors introduce the co-occurrence smoothing algorithm, which enables accurate recognition even with very limited training data. Since the results were evaluated on a standard database, they can be used as benchmarks to evaluate future systems},
annote = {Phoneme table is in this paper!},
author = {Lee, K F and Hon, H W},
doi = {10.1109/29.46546},
file = {:home/moritz/Documents/Mendeley/Lee, Hon/Acoustics, Speech and Signal Processing, IEEE Transactions on/Lee, Hon - 1989 - Speaker-independent phone recognition using hidden Markov models.pdf:pdf},
issn = {0096-3518},
journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
number = {11},
pages = {1641--1648},
title = {{Speaker-independent phone recognition using hidden Markov models}},
volume = {37},
year = {1989}
}
@misc{olah2016attention,
author = {Olah, Chris and Carter, Shan},
howpublished = {http://distill.pub/2016/augmented-rnns/},
title = {{Attention and Augmented Recurrent Neural Networks}},
year = {2016}
}
@book{Oliveira2006,
author = {Oliveira, Suely and Stewart, David},
file = {:home/moritz/Documents/Mendeley/Oliveira, Stewart/Unknown/Oliveira, Stewart - 2006 - Writing Scientific Software.pdf:pdf},
isbn = {9780521858960},
pages = {292},
publisher = {Cambridge Cambridge university press},
title = {{Writing Scientific Software}},
year = {2006}
}
@article{Palaz2015,
abstract = {Automatic speech recognition systems typically model the rela-tionship between the acoustic speech signal and the phones in two separate steps: feature extraction and classifier training. In our recent works, we have shown that, in the framework of con-volutional neural networks (CNN), the relationship between the raw speech signal and the phones can be directly modeled and ASR systems competitive to standard approach can be built. In this paper, we first analyze and show that, between the first two convolutional layers, the CNN learns (in parts) and models the phone-specific spectral envelope information of 2-4 ms speech. Given that we show that the CNN-based approach yields ASR trends similar to standard short-term spectral based ASR sys-tem under mismatched (noisy) conditions, with the CNN-based approach being more robust.},
author = {Palaz, Dimitri and Magimai-Doss, Mathew and Collobert, Ronan},
file = {:home/moritz/Documents/Mendeley/Palaz, Magimai-Doss, Collobert/Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH/Palaz, Magimai-Doss, Collobert - 2015 - Analysis of CNN-based speech recognition system using raw speech as input.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Automatic speech recognition,Convolutional neural networks,Raw signal,Robust speech recognition},
pages = {11--15},
title = {{Analysis of CNN-based speech recognition system using raw speech as input}},
volume = {2015-January},
year = {2015}
}
@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/moritz/Documents/Mendeley/Pascanu, Mikolov, Bengio/Proceedings of The 30th International Conference on Machine Learning/Pascanu, Mikolov, Bengio - 2012 - Understanding the exploding gradient problem.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {Proceedings of The 30th International Conference on Machine Learning},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{Understanding the exploding gradient problem}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}
@book{Rojas1996,
author = {Rojas, Raul},
doi = {10.1109/78.127967},
file = {:home/moritz/Documents/Mendeley/Rojas/Unknown/Rojas - 1996 - Neural Networks - A Systematic Introduction - Backpropagation.pdf:pdf},
isbn = {9783540605058},
issn = {1053587X},
pages = {152 -- 184},
publisher = {Springer Berlin Heidelberg},
title = {{Neural Networks - A Systematic Introduction - Backpropagation}},
url = {https://page.mi.fu-berlin.de/rojas/neural/neuron.pdf},
year = {1996}
}
@article{Sak2014,
abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1128v1},
author = {Sak, Ha$\backslash$csim and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
doi = {arXiv:1402.1128},
eprint = {arXiv:1402.1128v1},
file = {:home/moritz/Documents/Mendeley/Sak, Senior, Beaufays/arXiv preprint arXiv1402.1128/Sak, Senior, Beaufays - 2014 - Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recogniti.pdf:pdf},
journal = {arXiv preprint arXiv:1402.1128},
number = {Cd},
title = {{Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}},
year = {2014}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, over tting is a serious problem in such networks. Large networks are also slow to use, making it dicult to deal with over tting by combining the predictions of many di erent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of di erent $\backslash$thinned" networks. At test time, it is easy to approximate the e ect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signi cantly reduces over tting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classi cation and computational biology, obtaining state of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:home/moritz/Documents/Mendeley/Srivastava et al/Journal of Machine Learning Research/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Over tting.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Over tting}},
url = {http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf},
volume = {15},
year = {2014}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
eprint = {1409.3215},
file = {:home/moritz/Documents/Mendeley/Sutskever, Vinyals, Le/Advances in Neural Information Processing Systems (NIPS)/Sutskever, Vinyals, Le - 2014 - Sequence to sequence learning with neural networks.pdf:pdf},
isbn = {1409.3215},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {3104--3112},
title = {{Sequence to sequence learning with neural networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/moritz/Documents/Mendeley/Tobergte, Curtis/Journal of Chemical Information and Modeling/Tobergte, Curtis - 2013 - CONVOLUTIONAL, LONG SHORT-TERM MEMORY, FULLY CONNECTED DEEP NEURAL NETWORKS.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{CONVOLUTIONAL, LONG SHORT-TERM MEMORY, FULLY CONNECTED DEEP NEURAL NETWORKS}},
volume = {53},
year = {2013}
}
@misc{VanRossum2001,
abstract = {This document gives coding conventions for the Python code comprising the standard library in the main Python distribution. Please see the companion informational PEP describing style guidelines for the C code in the C implementation of Python},
author = {van Rossum, Guido and Warsaw, Barry and Coghlan, Nick},
title = {{PEP 8 -- Style Guide for Python Code}},
url = {https://www.python.org/dev/peps/pep-0008/},
urldate = {2001-01-01},
year = {2001}
}
@article{Wager2013,
abstract = {Dropout and other feature noising schemes control overfitting by artificially cor- rupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regular- izer is first-order equivalent to an L 2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learning algorithm, and find that a close rel- ative of AdaGrad operates by repeatedly solving linear dropout-regularized prob- lems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We ap- ply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.},
author = {Wager, Stefan and Wang, Sida and Liang, Percy and Science, Computer},
file = {:home/moritz/Documents/Mendeley/Wager et al/Advances in Neural Information Processing Systems/Wager et al. - 2013 - Dropout Training as Adaptive Regularization.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {351----359},
title = {{Dropout Training as Adaptive Regularization}},
url = {http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf},
volume = {26},
year = {2013}
}
@article{Werbos2006,
abstract = {Backwards calculation of derivatives -- sometimes called the reverse$\backslash$nmode, the full adjoint method, or backpropagation -- has been developed$\backslash$nand applied in many fields. This paper reviews several strands of$\backslash$nhistory, advanced capabilities and types of application -- particularly$\backslash$nthose which are crucial to the development of brain-like capabilities$\backslash$nin intelligent control and artificial intelligence.},
author = {Werbos, Paul J.},
doi = {10.1007/3-540-28438-9_2},
file = {:home/moritz/Documents/Mendeley/Werbos/Lecture Notes in Computational Science and Engineering/Werbos - 2006 - Backwards Differentiation in AD and Neural Nets Past Links and New Opportunities.pdf:pdf},
isbn = {9783540284031},
issn = {14397358},
journal = {Lecture Notes in Computational Science and Engineering},
keywords = {Adjoint,Approximate dynamic programming,Backpropagation,Implicit systems,Intelligent control,MLP,Neural networks,Recurrent networks,Reinforcement learning,Reverse mode},
pages = {15--34},
title = {{Backwards Differentiation in AD and Neural Nets: Past Links and New Opportunities}},
volume = {50},
year = {2006}
}
@misc{Woodland2006,
author = {Woodland, Phil and Evermann, Gunnar and Gales, Mark},
title = {{HTKBook}},
url = {http://www1.icsi.berkeley.edu/Speech/docs/HTKBook/node65{\%}7B{\_}{\%}7Did.html},
year = {2006}
}
